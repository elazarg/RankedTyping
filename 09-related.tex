\section{Related Work}
\label{sec:related}

\paragraph{Gradual typing and its foundations.}
Gradual typing provides a principled account of mixing static and dynamic typing \cite{SiekTaha2006}. Subsequent work articulated meta-theoretic criteria such as the \emph{Gradual Guarantee} and precision monotonicity \cite{SiekGarciaTaha2015}. \emph{Abstracting Gradual Typing (AGT)} recasts gradual typing via abstract interpretation, formulating a Galois connection between precise and gradual types and deriving typing and casts from abstraction/concretization \cite{GarciaClarkTanter2016}. More recently, \emph{Gradual Type Theory} axiomatizes casts, the dynamic type, and the gradual guarantee in a type-theoretic setting, supplying logical tools to reason about equivalence and optimization \cite{NewLicataAhmed2021}. Our proposal is deliberately cast-free and purely static: instead of a single collapsed unknown, we expose an \emph{unfolded} family (grades) that measures analysis imprecision without altering operational semantics.

\paragraph{AGT versus unfolded unknowns.}
AGT uses a \emph{single} unknown ($?$) whose meaning is given by concretization to a set of precise types; consistency and casts are derived systematically from the abstraction. In our system, there is a \emph{countable family} $\Any^{0},\Any^{1},\ldots$ indexing \emph{assumption depth} in a purely static abstract domain: same-grade joins are conservative LUBs, and grades increase only at base-demand sites (downcasts). There is a simple erasure map $\mathrm{erase}(X^{i})=X$ and $\mathrm{erase}(\Any^{i})=?$ that embeds our types into AGT’s gradual types; conversely, all $\Any^{i}$ lie over the same AGT unknown, so grades add provenance information orthogonal to AGT’s precision preorder. Thus our system can be viewed as an \emph{unfolded} static stratum of AGT (no runtime casts), with grades recording where/how many assumptions were forced.

\paragraph{Gradual effect and annotation systems.}
Generalizations beyond simple types include gradual type-and-effect systems \cite{BanadosSchwerterGarciaTanter2016} and frameworks for gradualizing annotated type systems \cite{ThiemannFennell2014}. These quantify imprecision via a \emph{single} unknown element ($?$) and a consistency relation. In contrast, our grades stratify imprecision arithmetically, recording how many assumptions were needed, while retaining a conventional join-semilattice discipline.

\paragraph{Blame calculi and static blame.}
The blame calculus connects gradual/hybrid typing with higher-order contracts and proves the blame theorem: when casts fail, responsibility lies on the less-precisely typed side \cite{WadlerFindler2009}; polymorphic extensions preserve these guarantees \cite{AhmedFindlerSiekWadler2011}. More recently, \emph{static blame} predicts potential blame without executing programs~\cite{SuCulpepper2024}. Our setting shares the diagnostic spirit but introduces neither casts nor contract boundaries; the grade is a static, flow-sensitive measure of precision loss at control-flow joins and later downcasts.

\paragraph{Soft and quasi-static typing.}
Soft typing \cite{CartwrightFagan1991} and quasi-static typing~\cite{Thatte1990} prefigure many aims of gradual typing: inferring permissive types and inserting checks as needed (often around a greatest element such as $\Omega$ or \textsf{Dyn}). By contrast, our analysis remains fully static and is not a source-to-source transformation; quantitative information is recorded as a grade in the abstract domain rather than materialized as runtime checks.

\paragraph{Typed/untyped interoperation via ``like'' types.}
Thorn’s \emph{like types} enable fine-grained interoperation between typed and untyped code while retaining optimization potential in typed regions~\cite{WrigstadEtAl2010}. This is complementary: it proposes language constructs and runtime checks for interoperability, whereas we supply a static accounting of precision loss that can inform where such boundaries matter.

\paragraph{Abstract interpretation.}
Methodologically, our analysis is a standard monotone, flow-sensitive abstract interpretation~\cite{CousotCousot1977}. The novelty lies in the abstract domain: an \emph{unfolded} per-program slice in which the conservative same-grade join is the LUB and grade increases only at finitely many base-demand sites (idempotent per site). Thus fixed points exist without bespoke widenings, and the diagnostic information (grades and culprit sites) is exposed rather than collapsed.

\paragraph{Path sensitivity and disjunctive completion.}
Precision at joins can be recovered by duplicating control-flow contexts rather than collapsing them: 
trace partitioning, disjunctive completion, and configurable program analysis (CPA) are standard ways to pay for path sensitivity where it matters~\cite{MauborgneRival2005,BeyerHenzinger2007}.
Our approach is orthogonal: we remain path-insensitive but \emph{quantify} and \emph{attribute} the assumptions forced by later uses.
Rather than keeping disjunctions, we collapse early and carry a small integral signal (\emph{grade}) plus culprit sites; this keeps fixed-point iteration cheap while producing actionable diagnostics.

\paragraph{Flow/occurrence typing and control-flow refinement.}
Occurrence typing as in Typed Racket refines variable types across branches and joins by inspecting guards, recovering precision without runtime checks~\cite{TobinHochstadtFelleisen2008}.
Industrial analyzers (e.g., TypeScript, Flow) employ related control-flow analysis with unions.
These systems aim to \emph{avoid} imprecision; our aim is to \emph{make residual imprecision explicit and localizable}.
Where occurrence typing succeeds, our grades stay low; when correlations are untracked (by design), grades expose how many use-site assumptions were needed and where they arose.

\paragraph{Taint analysis and quantitative information flow.}
Classic taint analysis propagates a binary ``tainted/untainted'' label from designated sources (e.g.\ user input) through dataflow to security-sensitive sinks~\cite{DenningDenning1977}.
Information-flow type systems generalize this to security lattices that enforce confidentiality and integrity policies~\cite{SabelfeldMyers2003}, and quantitative information flow (QIF) further extends the framework to measure \emph{how much} information leaks across a boundary~\cite{Smith2009}.
Our grades admit a \emph{taint reading}: coercion sites ($\Any^{i}\le X^{i+1}$) act as taint \emph{sources}, each introducing one level of speculative uncertainty; $\max$-propagation in operators ensures that results inherit the worst-case taint; and ghost culprit sets provide full provenance (which sources tainted a given value).
The key structural difference is that classic taint is \emph{binary} (tainted or not), whereas our taint is \emph{quantitative}: a natural number measuring the depth of the speculation chain.
Moreover, propagation via $\max$ is \emph{idempotent}---touching the same tainted source twice does not increase the grade---so grades measure the \emph{depth} of the speculation chain rather than a count of individual taint events.
This positions grades as a type-level analogue of security labels, but tracking \emph{analysis confidence} rather than information security.